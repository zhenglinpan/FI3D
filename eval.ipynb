{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the segment below first, then the following can run normally. What the hell is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.37157156,  2.14924841, -3.05180756, ...,  0.19422355,\n",
       "        -0.05954894, -0.12712566],\n",
       "       [-1.6478645 ,  0.97119381, -1.35586851, ...,  0.21376756,\n",
       "         0.05664569, -0.19794771],\n",
       "       [-0.0712567 ,  1.02115189,  0.4101954 , ..., -0.16195227,\n",
       "         0.01179378, -0.08877666],\n",
       "       ...,\n",
       "       [ 2.40670481, -1.86114916,  3.0128795 , ...,  0.06596056,\n",
       "        -0.08211737,  0.04486519],\n",
       "       [ 0.03742224, -7.77419999,  2.77032292, ..., -0.01122986,\n",
       "        -0.11048092, -0.17316445],\n",
       "       [ 1.97254642, -4.70035634,  1.17356833, ..., -0.04575332,\n",
       "        -0.22742945, -0.17202515]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gt = np.load(r'./data_fi3d/emb/emb_gt.npy')\n",
    "gen = np.load(r'./data_fi3d/emb/emb_gen_none_None.npy')\n",
    "genp = np.load(r'./data_fi3d/emb/emb_gen_gaussian_1.npy')\n",
    "\n",
    "gt = np.mean(gt, axis=1)\n",
    "gen = np.mean(gen, axis=1)\n",
    "genp = np.mean(genp, axis=1)\n",
    "\n",
    "gt = np.delete(gt, [487, 1894], axis=0)\n",
    "gen = np.delete(gen, [487, 1894], axis=0)\n",
    "genp = np.delete(genp, [487, 1894], axis=0)\n",
    "\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "\n",
    "def apply_pca(data, n_components=10):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce the dimensionality of the data.\n",
    "\n",
    "    Args:\n",
    "        data: numpy.ndarray, dataset to be transformed.\n",
    "        n_components: int, number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Transformed dataset with reduced dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    pca = IncrementalPCA(n_components=n_components, batch_size=16)\n",
    "    # pca = PCA(n_components=n_components)\n",
    "    data_reduced = pca.fit_transform(data)\n",
    "    \n",
    "    return data_reduced\n",
    "\n",
    "apply_pca(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating repeating_quater motions --------------> \n",
      "FID ----------------------->\n",
      "FIDp ----------------------->\n",
      "MW2 ----------------------->\n",
      "         PCA ----------------------->\n",
      "         GMM ----------------------->\n",
      "MW2p ----------------------->\n",
      "         PCA ----------------------->\n",
      "         GMM ----------------------->\n",
      "FID: 0.5587005615234375 -> 0.4398956298828125\n",
      "MW2: 0.12026208655477956 -> 0.12934075743994108\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import ot   # pip install POT if missing\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, gt, gen, genp):\n",
    "        gt, rows = self.del_nan(np.mean(gt, axis=1))\n",
    "        gen, _ = self.del_nan(np.mean(gen, axis=1), rows)\n",
    "        genp, _ = self.del_nan(np.mean(genp, axis=1), rows)\n",
    "        \n",
    "        # print(gt.shape, gen.shape, genp.shape)  # (4608, 512) (4608, 512)\n",
    "        \n",
    "        self.gt = gt\n",
    "        self.gen = gen\n",
    "        self.genp = genp\n",
    "\n",
    "    def eval(self):\n",
    "        print(\"FID ----------------------->\")\n",
    "        fid = self.fid(self.gt, self.gen)\n",
    "        print(\"FIDp ----------------------->\")\n",
    "        fidp = self.fid(self.gt, self.genp)\n",
    "        print(\"MW2 ----------------------->\")\n",
    "        mw2 = self.MW2(self.gt, self.gen)\n",
    "        print(\"MW2p ----------------------->\")\n",
    "        mw2p = self.MW2(self.gt, self.genp)\n",
    "        \n",
    "        print(f'FID: {fid} -> {fidp}')\n",
    "        print(f'MW2: {mw2} -> {mw2p}')\n",
    "        \n",
    "        return fid, fidp, mw2, mw2p\n",
    "    \n",
    "    def fid(self, real, fake):\n",
    "        \"\"\"\n",
    "        Frechet Inception Distance\n",
    "        \"\"\"\n",
    "        n = real.shape[0]\n",
    "\n",
    "        mean_real = np.mean(real, axis=0)[None, ...]\n",
    "        mean_fake = np.mean(fake, axis=0)[None, ...]\n",
    "\n",
    "        cov_fake_sum = fake.T @ fake - n * mean_fake.T @ mean_fake\n",
    "        cov_real_sum = real.T @ real - n * mean_real.T @ mean_real\n",
    "\n",
    "        cov_fake = cov_fake_sum / (n - 1)\n",
    "        cov_real = cov_real_sum / (n - 1)\n",
    "                \n",
    "        mu1, mu2, sig1, sig2 = mean_fake, mean_real, cov_fake, cov_real\n",
    "\n",
    "        mu1_tensor = torch.tensor(mu1)\n",
    "        mu2_tensor = torch.tensor(mu2)\n",
    "        sig1_tensor = torch.tensor(sig1)\n",
    "        sig2_tensor = torch.tensor(sig2) \n",
    "\n",
    "        a = (mu1_tensor - mu2_tensor).square().sum(dim=-1)\n",
    "        b = sig1_tensor.trace() + sig2_tensor.trace()\n",
    "        c = torch.linalg.eigvals(sig1_tensor @ sig2_tensor).sqrt().real.sum(dim=-1)\n",
    "\n",
    "        score = a + b - 2 * c\n",
    "\n",
    "        return score.item()\n",
    "    \n",
    "    def MW2(self, dist1, dist2, K=13):\n",
    "        \"\"\"\n",
    "        Compute the Wasserstein distance between two distributions, each represented as\n",
    "        a multivariate Gaussian Mixture Model (GMM).\n",
    "\n",
    "        Args:\n",
    "            dist1: the first distribution, shape (N, D)\n",
    "            dist2: the second distribution, shape (N, D)\n",
    "            K: int, number of components in the GMM\n",
    "        \n",
    "        Returns:\n",
    "            float: The Wasserstein-2 distance between the two GMMs.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply PCA to reduce the dimensionality of the data\n",
    "        print(\"         PCA ----------------------->\")\n",
    "        dist1_pca = self.apply_pca(dist1)\n",
    "        dist2_pca = self.apply_pca(dist2)\n",
    "        \n",
    "        print(\"         GMM ----------------------->\")\n",
    "        # Fit GMMs to the input distributions\n",
    "        gmm1 = GaussianMixture(n_components=K, random_state=0, max_iter=200).fit(dist1_pca)\n",
    "        gmm2 = GaussianMixture(n_components=K, random_state=0, max_iter=200).fit(dist2_pca)\n",
    "        \n",
    "        # Compute the pairwise Euclidean distances between the means of the GMM components\n",
    "        C = ot.dist(gmm1.means_, gmm2.means_, metric='euclidean')\n",
    "\n",
    "        # print(f'gmm1 weights: {gmm1.weights_}, gmm2 weights: {gmm2.weights_}, gmm1 means: {gmm1.means_}, gmm2 means: {gmm2.means_}')\n",
    "        \n",
    "        # Normalize the cost matrix to prevent numerical instability\n",
    "        C /= C.max()\n",
    "\n",
    "        # Compute the optimal transport plan using the Earth Mover's Distance (EMD) algorithm\n",
    "        gamma = ot.emd(gmm1.weights_, gmm2.weights_, C)\n",
    "\n",
    "        # Calculate the Wasserstein distance using the transport plan and the cost matrix\n",
    "        W2 = np.sum(gamma * C)\n",
    "\n",
    "        return W2\n",
    "    \n",
    "    def apply_pca(self, data, n_components=10):\n",
    "        \"\"\"\n",
    "        Apply PCA to reduce the dimensionality of the data.\n",
    "\n",
    "        Args:\n",
    "            data: numpy.ndarray, dataset to be transformed.\n",
    "            n_components: int, number of principal components to retain.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Transformed dataset with reduced dimensions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pca = PCA(n_components=n_components)\n",
    "        pca = IncrementalPCA(n_components=n_components, batch_size=16)\n",
    "        \n",
    "        data_reduced = pca.fit_transform(data)\n",
    "        \n",
    "        return data_reduced\n",
    "        \n",
    "    def del_nan(self, array, rm_rows=None):\n",
    "        if rm_rows == None:\n",
    "            rm_rows = []\n",
    "            if np.isnan(array).any():\n",
    "                for i in range(array.shape[0]):\n",
    "                    for j in range(array.shape[1]):\n",
    "                        if np.isnan(array[i][j]):\n",
    "                            rm_rows.append(i)\n",
    "                            break\n",
    "            array = np.delete(array, rm_rows, axis=0)\n",
    "        else:\n",
    "            array = np.delete(array, rm_rows, axis=0)\n",
    "                        \n",
    "        return array, rm_rows\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    emb_dir = './data_fi3d/emb'\n",
    "    flist = os.listdir(emb_dir)\n",
    "    perturbs = [f.strip('.npy').split('_')[-2] + '_' + f.strip('.npy').split('_')[-1] for f in flist if 'emb_gen' in f]\n",
    "    \n",
    "    gt = np.load('./data_fi3d/emb/emb_gt.npy')\n",
    "    gen = np.load('./data_fi3d/emb/emb_gen_none_None.npy')\n",
    "    \n",
    "    res_dict = {'method': [], 'FID': [], 'MW2': [], 'FIDp': [], 'MW2p': []}\n",
    "    for perturb in perturbs:\n",
    "        print(f'Evaluating {perturb} motions --------------> ')\n",
    "        genp = np.load(f'./data_fi3d/emb/emb_gen_{perturb}.npy')\n",
    "        \n",
    "        evaluator = Evaluator(gt, gen, genp)\n",
    "        fid, fidp, mw2, mw2p = evaluator.eval()\n",
    "        \n",
    "        res_dict['method'].append(perturb)\n",
    "        res_dict['FID'].append(fid)\n",
    "        res_dict['MW2'].append(mw2)\n",
    "        res_dict['FIDp'].append(fidp)\n",
    "        res_dict['MW2p'].append(mw2p)\n",
    "\n",
    "    df = pd.DataFrame(res_dict)\n",
    "    df.to_csv('eval_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computational efficiency test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating smoothing_(5, 1) motions --------------> \n",
      "FID: 0.5587005615234375 -> 2.466796875\n",
      "MW2: 0.12026208655477956 -> 0.29983442000055904\n",
      "Evaluating uniform_1 motions --------------> \n",
      "FID: 0.5587005615234375 -> 6.2283172607421875\n",
      "MW2: 0.12026208655477956 -> 0.30102321897457995\n",
      "Evaluating shuffling_20 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.7993545532226562\n",
      "MW2: 0.12026208655477956 -> 0.1507211355209278\n",
      "Evaluating skipping_5 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.5721282958984375\n",
      "MW2: 0.12026208655477956 -> 0.12339909020452275\n",
      "Evaluating gaussian_1 motions --------------> \n",
      "FID: 0.5587005615234375 -> 1.79193115234375\n",
      "MW2: 0.12026208655477956 -> 0.290033033953415\n",
      "Evaluating smoothing_(1, 1) motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.6676406860351562\n",
      "MW2: 0.12026208655477956 -> 0.12299645395377269\n",
      "Evaluating gaussian_0.01 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.53924560546875\n",
      "MW2: 0.12026208655477956 -> 0.12014404223775903\n",
      "Evaluating skipping_50 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.554718017578125\n",
      "MW2: 0.12026208655477956 -> 0.13633155427289936\n",
      "Evaluating repeating_quater motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.4398956298828125\n",
      "MW2: 0.12026208655477956 -> 0.12934075743994108\n",
      "Evaluating smoothing_(10, 1) motions --------------> \n",
      "FID: 0.5587005615234375 -> 4.275016784667969\n",
      "MW2: 0.12026208655477956 -> 0.3123322266994141\n",
      "Evaluating skipping_10 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.56884765625\n",
      "MW2: 0.12026208655477956 -> 0.12798157302525012\n",
      "Evaluating skipping_20 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.5845260620117188\n",
      "MW2: 0.12026208655477956 -> 0.1411508101325802\n",
      "Evaluating shuffling_50 motions --------------> \n",
      "FID: 0.5587005615234375 -> 1.5434799194335938\n",
      "MW2: 0.12026208655477956 -> 0.15884209025950607\n",
      "Evaluating uniform_0.01 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.5399856567382812\n",
      "MW2: 0.12026208655477956 -> 0.11800574678519393\n",
      "Evaluating none_None motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.5587005615234375\n",
      "MW2: 0.12026208655477956 -> 0.12026208655477956\n",
      "Evaluating skipping_30 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.555877685546875\n",
      "MW2: 0.12026208655477956 -> 0.12803548170076695\n",
      "Evaluating uniform_0.1 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.44588470458984375\n",
      "MW2: 0.12026208655477956 -> 0.1371444596070937\n",
      "Evaluating gaussian_0.001 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.5518722534179688\n",
      "MW2: 0.12026208655477956 -> 0.12025938505753414\n",
      "Evaluating shuffling_10 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.6253128051757812\n",
      "MW2: 0.12026208655477956 -> 0.13635821901618522\n",
      "Evaluating uniform_0.001 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.5655593872070312\n",
      "MW2: 0.12026208655477956 -> 0.12010177547262083\n",
      "Evaluating smoothing_(3, 1) motions --------------> \n",
      "FID: 0.5587005615234375 -> 1.4301223754882812\n",
      "MW2: 0.12026208655477956 -> 0.2883421930279184\n",
      "Evaluating shuffling_30 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.99591064453125\n",
      "MW2: 0.12026208655477956 -> 0.14886197575572846\n",
      "Evaluating gaussian_0.1 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.4887237548828125\n",
      "MW2: 0.12026208655477956 -> 0.13853247001355706\n",
      "Evaluating repeating_half motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.51507568359375\n",
      "MW2: 0.12026208655477956 -> 0.12313324552646226\n",
      "Evaluating shuffling_5 motions --------------> \n",
      "FID: 0.5587005615234375 -> 0.6144943237304688\n",
      "MW2: 0.12026208655477956 -> 0.13666280147817267\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import ot   # pip install POT if missing\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, gt, gen, genp):\n",
    "        gt, rows = self.del_nan(np.mean(gt, axis=1))\n",
    "        gen, _ = self.del_nan(np.mean(gen, axis=1), rows)\n",
    "        genp, _ = self.del_nan(np.mean(genp, axis=1), rows)\n",
    "        \n",
    "        # print(gt.shape, gen.shape, genp.shape)  # (4608, 512) (4608, 512)\n",
    "        \n",
    "        self.gt = gt\n",
    "        self.gen = gen\n",
    "        self.genp = genp\n",
    "\n",
    "    def eval(self):\n",
    "        start_time = time.time()\n",
    "        # print(\"FID ----------------------->\")\n",
    "        fid = self.fid(self.gt, self.gen)\n",
    "        # print(\"FIDp ----------------------->\")\n",
    "        fidp = self.fid(self.gt, self.genp)\n",
    "        efid = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        gt_pca = self.apply_pca(self.gt)    # PCA 最好放在MW2里面  这里放在外面是为了计算时间\n",
    "        gen_pca = self.apply_pca(self.gen)\n",
    "        gt_pca = self.apply_pca(self.gt)\n",
    "        genp_pca = self.apply_pca(self.genp)\n",
    "        epca = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # print(\"MW2 ----------------------->\")\n",
    "        mw2 = self.MW2(gt_pca, gen_pca)\n",
    "        # print(\"MW2p ----------------------->\")\n",
    "        mw2p = self.MW2(gt_pca, genp_pca)\n",
    "        emw2 = time.time() - start_time\n",
    "        \n",
    "        print(f'FID: {fid} -> {fidp}')\n",
    "        print(f'MW2: {mw2} -> {mw2p}')\n",
    "        \n",
    "        return fid, fidp, mw2, mw2p, efid, epca, emw2\n",
    "    \n",
    "    def fid(self, real, fake):\n",
    "        \"\"\"\n",
    "        Frechet Inception Distance\n",
    "        \"\"\"\n",
    "        n = real.shape[0]\n",
    "\n",
    "        mean_real = np.mean(real, axis=0)[None, ...]\n",
    "        mean_fake = np.mean(fake, axis=0)[None, ...]\n",
    "\n",
    "        cov_fake_sum = fake.T @ fake - n * mean_fake.T @ mean_fake\n",
    "        cov_real_sum = real.T @ real - n * mean_real.T @ mean_real\n",
    "\n",
    "        cov_fake = cov_fake_sum / (n - 1)\n",
    "        cov_real = cov_real_sum / (n - 1)\n",
    "                \n",
    "        mu1, mu2, sig1, sig2 = mean_fake, mean_real, cov_fake, cov_real\n",
    "\n",
    "        mu1_tensor = torch.tensor(mu1)\n",
    "        mu2_tensor = torch.tensor(mu2)\n",
    "        sig1_tensor = torch.tensor(sig1)\n",
    "        sig2_tensor = torch.tensor(sig2) \n",
    "\n",
    "        a = (mu1_tensor - mu2_tensor).square().sum(dim=-1)\n",
    "        b = sig1_tensor.trace() + sig2_tensor.trace()\n",
    "        c = torch.linalg.eigvals(sig1_tensor @ sig2_tensor).sqrt().real.sum(dim=-1)\n",
    "\n",
    "        score = a + b - 2 * c\n",
    "\n",
    "        return score.item()\n",
    "    \n",
    "    def MW2(self, dist1_pca, dist2_pca, K=13):\n",
    "        \"\"\"\n",
    "        Compute the Wasserstein distance between two distributions, each represented as\n",
    "        a multivariate Gaussian Mixture Model (GMM).\n",
    "\n",
    "        Args:\n",
    "            dist1: the first distribution, shape (N, D)\n",
    "            dist2: the second distribution, shape (N, D)\n",
    "            K: int, number of components in the GMM\n",
    "        \n",
    "        Returns:\n",
    "            float: The Wasserstein-2 distance between the two GMMs.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply PCA to reduce the dimensionality of the data\n",
    "        # print(\"         PCA ----------------------->\")\n",
    "        # dist1_pca = self.apply_pca(dist1)\n",
    "        # dist2_pca = self.apply_pca(dist2)\n",
    "        \n",
    "        # print(\"         GMM ----------------------->\")\n",
    "        # Fit GMMs to the input distributions\n",
    "        gmm1 = GaussianMixture(n_components=K, random_state=0, max_iter=200).fit(dist1_pca)\n",
    "        gmm2 = GaussianMixture(n_components=K, random_state=0, max_iter=200).fit(dist2_pca)\n",
    "        \n",
    "        # Compute the pairwise Euclidean distances between the means of the GMM components\n",
    "        C = ot.dist(gmm1.means_, gmm2.means_, metric='euclidean')\n",
    "\n",
    "        # print(f'gmm1 weights: {gmm1.weights_}, gmm2 weights: {gmm2.weights_}, gmm1 means: {gmm1.means_}, gmm2 means: {gmm2.means_}')\n",
    "        \n",
    "        # Normalize the cost matrix to prevent numerical instability\n",
    "        C /= C.max()\n",
    "\n",
    "        # Compute the optimal transport plan using the Earth Mover's Distance (EMD) algorithm\n",
    "        gamma = ot.emd(gmm1.weights_, gmm2.weights_, C)\n",
    "\n",
    "        # Calculate the Wasserstein distance using the transport plan and the cost matrix\n",
    "        W2 = np.sum(gamma * C)\n",
    "\n",
    "        return W2\n",
    "    \n",
    "    def apply_pca(self, data, n_components=10):\n",
    "        \"\"\"\n",
    "        Apply PCA to reduce the dimensionality of the data.\n",
    "\n",
    "        Args:\n",
    "            data: numpy.ndarray, dataset to be transformed.\n",
    "            n_components: int, number of principal components to retain.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Transformed dataset with reduced dimensions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # pca = PCA(n_components=n_components)\n",
    "        pca = IncrementalPCA(n_components=n_components, batch_size=16)\n",
    "        \n",
    "        data_reduced = pca.fit_transform(data)\n",
    "        \n",
    "        return data_reduced\n",
    "        \n",
    "    def del_nan(self, array, rm_rows=None):\n",
    "        if rm_rows == None:\n",
    "            rm_rows = []\n",
    "            if np.isnan(array).any():\n",
    "                for i in range(array.shape[0]):\n",
    "                    for j in range(array.shape[1]):\n",
    "                        if np.isnan(array[i][j]):\n",
    "                            rm_rows.append(i)\n",
    "                            break\n",
    "            array = np.delete(array, rm_rows, axis=0)\n",
    "        else:\n",
    "            array = np.delete(array, rm_rows, axis=0)\n",
    "                        \n",
    "        return array, rm_rows\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    emb_dir = './data_fi3d/emb'\n",
    "    flist = os.listdir(emb_dir)\n",
    "    perturbs = [f.strip('.npy').split('_')[-2] + '_' + f.strip('.npy').split('_')[-1] for f in flist if 'emb_gen' in f]\n",
    "    \n",
    "    gt = np.load('./data_fi3d/emb/emb_gt.npy')\n",
    "    gen = np.load('./data_fi3d/emb/emb_gen_none_None.npy')\n",
    "    \n",
    "    res_dict = {'method': [], 'FID': [], 'MW2': [], 'FIDp': [], 'MW2p': [], 'time_fid': [], 'time_pca':[], 'time_mw2': []}\n",
    "    for perturb in perturbs:\n",
    "        print(f'Evaluating {perturb} motions --------------> ')\n",
    "        genp = np.load(f'./data_fi3d/emb/emb_gen_{perturb}.npy')\n",
    "        \n",
    "        evaluator = Evaluator(gt, gen, genp)\n",
    "        fid, fidp, mw2, mw2p, time_fid, time_pca, time_mw2 = evaluator.eval()\n",
    "        \n",
    "        res_dict['method'].append(perturb)\n",
    "        res_dict['FID'].append(fid)\n",
    "        res_dict['MW2'].append(mw2)\n",
    "        res_dict['FIDp'].append(fidp)\n",
    "        res_dict['MW2p'].append(mw2p)\n",
    "        res_dict['time_fid'].append(time_fid)\n",
    "        res_dict['time_pca'].append(time_pca)\n",
    "        res_dict['time_mw2'].append(time_mw2)\n",
    "\n",
    "    df = pd.DataFrame(res_dict)\n",
    "    df.to_csv('eval_results_time.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
